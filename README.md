
# 🧠 Dominick Dominican

> Independent AI Safety Researcher • Multilingual LLM Evaluator • Prompt Robustness Explorer  
> 📍 Based in Earth | 🌐 Advocating Responsible AI in All Languages

---

### 👋 About Me

Hi, I’m Dominick — a researcher passionate about **multilingual alignment**, **LLM safety**, and **prompt injection mitigation**.  
My work focuses on evaluating how large language models behave across linguistically and culturally diverse scenarios, especially in **safety-critical domains** like healthcare, law, and education.

🔬 I build open-source evaluation tools and design adversarial benchmarks that test the limits of today’s most capable LLMs.

---

### 🔭 Current Project

🚧 [`SafeLLM-Multilingual-Eval`](https://github.com/DominickDominican/SafeLLM-Multilingual-Eval):  
A cross-language evaluation framework that analyzes safety, hallucination, and alignment variance in LLMs like Claude, GPT-4, and Mistral across over a dozen languages.  
→ Focus areas: prompt injections, semantic consistency, low-resource risk

---

### 🧰 Tech Stack & Interests

- ⚙️ Python, JSONL, Shell scripting  
- 🔍 OpenAI API, Claude API, Mistral  
- 📊 Data Viz: matplotlib, seaborn  
- 🌍 NLP, Evaluation, AI Alignment

---

### 📬 Let's Connect

- 📧 Email: dominickdominican47@gmail.com  
- 🌐 GitHub: [@DominickDominican](https://github.com/DominickDominican)

---

### 📝 Fun Fact

> I believe the path to *universal alignment* begins by evaluating language models where they’re weakest — **in the languages they least expect.**
